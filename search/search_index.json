{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to my XLA notes. These notes provide you a quick overview on XLA compiler.</p> <p>Happy Learning :)</p> <p></p> <p>Get in touch: </p>"},{"location":"callGraph/","title":"Call Graph","text":"<pre><code>sequenceDiagram\n\n    actor user\n\n    user -&gt;&gt; xla_builder: XlaBuilder::Build\n    xla_builder --&gt;&gt; user: XlaComputation\n    user -&gt;&gt; client: ExecuteAndTransfer (XlaComputation)\n    client -&gt;&gt; service: Execute (XlaComputation, ExecutionOptions)\n    service -&gt;&gt; backend: GetExecutors (ExecutionOptions)\n    backend --&gt;&gt; service: StreamExecutor\n    service -&gt;&gt; cpu_compiler: Compile (HloModule, StreamExecutor)\n    cpu_compiler -&gt;&gt; llvm_compiler: Compile (HloModule, StreamExecutor)\n    llvm_compiler -&gt;&gt; cpu_compiler: RunHloPasses (HloModule)\n    cpu_compiler --&gt;&gt; llvm_compiler: HloModule\n    llvm_compiler -&gt;&gt;+ cpu_compiler: RunBackend (HloModule)\n    cpu_compiler -&gt;&gt; hlo_memory_scheduler: ScheduleModule (HloModule)\n    hlo_memory_scheduler --&gt;&gt; cpu_compiler: HloSchedule\n    cpu_compiler -&gt;&gt; buffer_assignment: BufferAssigner::Run (HloModule, HloSchedule)\n    buffer_assignment --&gt;&gt; cpu_compiler: BufferAssignment\n    cpu_compiler -&gt;&gt; ir_emitter: IrEmitter::EmitComputation (HloModule, HloSchedule)\n    ir_emitter --&gt;&gt; cpu_compiler: llvm::Function\n    cpu_compiler -&gt;&gt; cpu_executable: CpuExecutable::Create (jit, BufferAssignment, HloModule, llvm::Function)\n    cpu_executable --&gt;&gt; cpu_compiler: CpuExecutable\n    cpu_compiler --&gt;&gt;- llvm_compiler: Executable\n    llvm_compiler --&gt;&gt; cpu_compiler: Executable\n    cpu_compiler --&gt;&gt; service: Executable\n    service -&gt;&gt; executable: ExecuteOnStreams (Executable)\n    executable --&gt;&gt; service: GlobalDataHandle\n    service --&gt;&gt; client: GlobalDataHandle\n    client --&gt;&gt;  user: GlobalData\n</code></pre>"},{"location":"callGraph/#how-to-read-the-call-graph","title":"How to read the Call Graph?","text":"<pre><code>sequenceDiagram\n\nfilename_1 -&gt;&gt; filename_2: functionCalled (parameter1, parameter2, ..)\nfilename_2 --&gt;&gt; filename_1: returnedVariable\n</code></pre>"},{"location":"debugger/","title":"Setup Debugger","text":""},{"location":"debugger/#clone-and-build-xla","title":"Clone and build XLA","text":"<p>To clone XLA <pre><code>git clone https://github.com/openxla/xla.git\n</code></pre> XLA uses docker. To create and configure xla docker <pre><code>docker run --name xla -w /xla -it -d --rm -v $PWD:/xla tensorflow/build:latest-python3.9 bash\ndocker exec xla ./configure\n</code></pre> This creates a docker container named <code>xla</code>. To start this container <pre><code>docker exec -it xla bash\n</code></pre> Once inside container, to build XLA <pre><code>bazel build --config=dbg --test_output=all --spawn_strategy=sandboxed //xla/...\n</code></pre> Your XLA is now built.</p> <p>Note</p> <p>Using <code>--config=dbg</code> flag to add debugging symbols. This flag is required by the debugger.</p>"},{"location":"debugger/#compile-a-unit-test","title":"Compile a unit test","text":"<p>Best way to learning about a codebase is to run its unit tests through debugger.</p> <p>To compile a unit test <pre><code>bazel test --config=dbg //xla/tests:tuple_test_cpu\n</code></pre> This will create an executable <pre><code>bazel-bin/xla/tests/tuple_test_cpu\n</code></pre> Test it by executing it.</p>"},{"location":"debugger/#setup-visual-studio-code","title":"Setup Visual Studio Code","text":"<p>I use VSCode debugger to debug the executable.</p> <p>To debug in VSCode, install following extensions:</p> <ul> <li>C/C++</li> <li>C/C++ Extension Pack</li> <li>Dev Containers</li> </ul> <p>Next, add the following lines to <code>.vscode/launch.json</code> <pre><code>{\n  \"configurations\": [\n  {\n    \"name\": \"(gdb) Launch\",\n    \"type\": \"cppdbg\",\n    \"request\": \"launch\",\n    \"program\": \"${workspaceFolder}/bazel-bin/xla/tests/tuple_test_cpu\",\n    \"args\": [],\n    \"stopAtEntry\": false,\n    \"cwd\": \"${workspaceFolder}\",\n    \"environment\": [],\n    \"externalConsole\": false,\n    \"MIMode\": \"gdb\",\n    \"setupCommands\": [\n      {\n        \"description\": \"Enable pretty-printing for gdb\",\n        \"text\": \"-enable-pretty-printing\",\n        \"ignoreFailures\": true\n      }\n    ]\n  }\n  ]\n}\n</code></pre></p> <p>Note</p> <p>The line highlighted above says VSCode will run debugger on <code>tuple_test_cpu</code>. You can replace <code>\"program\"</code> field with any executable you wish to debug.</p> <p>Because XLA uses docker container to execute, we need to attach docker container to VSCode before hitting debug.</p> <p>To attach a running container to VSCode, press <code>Ctrl + Shift + P</code> &gt; type \"attach to running container\" &gt; select your running xla container (the one that you had started by <code>docker exec -it xla bash</code>).</p> <p></p> <p>Add breakpoints where required and press F5 to start debugging.</p>"},{"location":"debugger/#references","title":"References","text":"<p>To clone and build XLA: https://github.com/openxla/xla/blob/main/docs/developer_guide.md</p> <p>To setup debugger: https://nekodaemon.com/2021/08/04/Easy-way-to-debug-TensorFlow-XLA-Compiler-using-VSCode/</p>"},{"location":"structure/","title":"XLA Structure","text":"<pre><code>graph TD;\n    XLA --&gt; HloModule;\n    HloModule --&gt; HloComputation;\n    HloComputation --&gt; HloInstruction;\n    HloInstruction --&gt; Shape;\n    Shape --&gt; Layout;\n\n    XLA --&gt; HloSchedule;\n    HloSchedule --&gt; HloInstructionSequence;\n\n    XLA --&gt; XlaBuilder;\n    XlaBuilder --&gt; ProgramShape;\n    XlaBuilder --&gt; XlaComputation;\n    XlaBuilder --&gt; XlaOp;\n\n    XLA --&gt; HloEvaluator;\n    HloEvaluator --&gt; Literal;\n\n    XLA --&gt; Client;\n    Client --&gt; Service;\n    Service --&gt; Executable;\n    Executable --&gt; Stream;\n    Stream --&gt; StreamExecutor;\n    StreamExecutor --&gt; Kernel;\n\n    XLA --&gt; BufferAssignment;\n    BufferAssignment --&gt; BufferAllocation;\n\n\n\n    click HloModule \"../structure/HloModule\"\n    click HloComputation \"../structure/HloComputation\"\n    click HloInstruction \"../structure/HloInstruction\"\n    click Shape \"../structure/Shape\"\n    click Layout \"../structure/Layout\"\n\n    click HloSchedule \"../structure/HloSchedule\"\n    click HloInstructionSequence \"../structure/HloInstructionSequence\"\n\n    click XlaBuilder \"../structure/XlaBuilder\"\n    click ProgramShape \"../structure/ProgramShape\"\n    click XlaComputation \"../structure/XlaComputation\"\n    click XlaOp \"../structure/XlaOp\"\n\n    click HloEvaluator \"../structure/HloEvaluator\"\n    click Literal \"../structure/Literal\"\n\n    click Client \"../structure/Client\"\n    click Service \"../structure/Service\"\n    click Executable \"../structure/Executable\"\n    click Stream \"../structure/Stream\"\n    click StreamExecutor \"../structure/StreamExecutor\"\n    click Kernel \"../structure/Kernel\"\n\n    click BufferAssignment \"../structure/BufferAssignment\"\n    click BufferAllocation \"../structure/BufferAllocation\"</code></pre>"},{"location":"structure/#how-to-read-the-xla-structure","title":"How to read the XLA Structure?","text":"<pre><code>graph TD;\n    Class1 --&gt; |Uses|Class2</code></pre> <p>Example: <code>HloInstruction</code> Uses <code>Shape</code> to represent the shape of its result.</p> <p>Note</p> <p>Not all the \"Uses\" relation is captured here. Relations like \"<code>HloInstructionSequence</code> Uses <code>HloInstruction</code> to represent instructions present in sequence\" is not captured as this is \"expected\". This was done to keep the graph clean.</p>"},{"location":"structure/BufferAllocation/","title":"BufferAllocation","text":"<p>This class abstracts an allocation of contiguous memory which can hold the values described by LogicalBuffers. Each LogicalBuffer occupies a sub-range of the allocation, represented by a Slice. A single BufferAllocation may hold LogicalBuffers with disjoint liveness, which may have overlapping Slices. A single BufferAllocation may also hold LogicalBuffers with overlapping liveness, which must have disjoint Slices.</p> <p>Example:  BufferAllocation (contiguous memory):</p> <p>[ -- [LogicalBuffer1 (Slice1)] -------- [LogicalBuffer2 (Slice2)] ---------------- [LogicalBuffer3 (Slice3)] ---- ]</p>"},{"location":"structure/BufferAssignment/","title":"BufferAssignment","text":"<p>This class encapsulates an assignment of the LogicalBuffers in an XLA module to a set of BufferAllocations.</p>"},{"location":"structure/BufferAssignment/#attributes-overview","title":"Attributes Overview","text":"<ul> <li>Stores relation between LogicalBuffers and BufferAllocations</li> </ul> <pre><code>  // The vector of buffer allocations. Indexed by BufferAllocation::Index.\n  std::vector&lt;BufferAllocation&gt; allocations_;\n\n  // Maps Buffers to the index of the BufferAllocation which holds the buffer.\n  absl::flat_hash_map&lt;const HloValue*, BufferAllocation::Index&gt;\n      allocation_index_for_value_;\n\n  const HloModule* module_;\n\n  const std::unique_ptr&lt;HloOrdering&gt; hlo_ordering_;\n</code></pre>"},{"location":"structure/BufferAssignment/#functions-overview","title":"Functions Overview","text":"<ul> <li>Perform operations over BufferAllocation</li> </ul> <pre><code>  // Returns whether the given buffer has been assigned an allocation.\n  bool HasAllocation(const HloValue&amp; value) const;\n\n  // Returns the allocation that a particular LogicalBuffer has been assigned\n  // to. CHECKs if buffer has not been assigned an allocation.\n  const BufferAllocation&amp; GetAssignedAllocation(const HloValue&amp; value) const;\n\n  // Adds a LogicalBuffer to the set assigned to the given allocation.\n  void AddAssignment(BufferAllocation* allocation, const HloBuffer&amp; buffer,\n                     int64_t offset, int64_t size);\n</code></pre>"},{"location":"structure/Client/","title":"Client","text":"<p>XLA service's client object -- wraps the service with convenience and lifetime-oriented methods.</p>"},{"location":"structure/Client/#attributes-overview","title":"Attributes Overview","text":"<ul> <li>Interface to Service</li> </ul> <pre><code>  ServiceInterface* stub_;  // Stub that this client is connected on.\n</code></pre>"},{"location":"structure/Client/#functions-overview","title":"Functions Overview","text":"<ul> <li>Request to Compile and Execute XlaComputation on Service</li> <li>Transfer data to and from Service</li> </ul> <pre><code>  // Compile the computation with the given argument shapes and returns the\n  // handle to the compiled executable. The compiled executable is cached on the\n  // service, and the returned handle can be used for execution without\n  // re-compile.\n  StatusOr&lt;ExecutionHandle&gt; Compile(\n      const XlaComputation&amp; computation,\n      absl::Span&lt;const Shape&gt; argument_shapes,\n      const ExecutionOptions* execution_options = nullptr);\n\n  // Executes the compiled executable for the given handle with the given\n  // arguments and returns the global data that was produced from the execution.\n  StatusOr&lt;std::unique_ptr&lt;GlobalData&gt;&gt; Execute(\n      const ExecutionHandle&amp; handle, absl::Span&lt;GlobalData* const&gt; arguments,\n      ExecutionProfile* execution_profile = nullptr);\n\n  // Transfer the global data provided to this client process, which is\n  // returned in the provided literal.\n  StatusOr&lt;Literal&gt; Transfer(const GlobalData&amp; data,\n                             const Shape* shape_with_layout = nullptr);\n\n  // Transfer the given literal to the server. This allocates memory on the\n  // device and copies the literal's contents over. Returns a global data handle\n  // that can be used to refer to this value from the client.\n  StatusOr&lt;std::unique_ptr&lt;GlobalData&gt;&gt; TransferToServer(\n      const LiteralSlice&amp; literal, const DeviceHandle* device_handle = nullptr);\n</code></pre> <p>Note</p> <ul> <li>Literal is data on host</li> <li>GlobalData is data on device</li> </ul>"},{"location":"structure/Executable/","title":"Executable","text":"<p>A given platform's compiler will produce an Executable -- this is a uniform interface that is used for launching compiled programs across platforms</p> <p>Note</p> <ul> <li>This is where actual execution of Ops takes place</li> </ul>"},{"location":"structure/Executable/#attributes-overview","title":"Attributes Overview","text":"<pre><code>  // HloModule this was compiled from. BufferAssignment keeps pointers to\n  // HloInstructions owned by the HloModule so we need to keep the HloModule\n  // around if we keep the BufferAssignment around.\n  //\n  // This member may be nullptr, if the given executable type doesn't need it\n  // for execution.\n  const std::shared_ptr&lt;HloModule&gt; hlo_module_;\n\n  // The serialized HLO proto. Non-null only if dumping snapshots is enabled.\n  std::unique_ptr&lt;HloProto const&gt; hlo_proto_;\n</code></pre>"},{"location":"structure/Executable/#functions-overview","title":"Functions Overview","text":"<pre><code>  // Enqueues the compilation result on the provided stream, passing the given\n  // arguments. This call is blocking and returns after the execution is done.\n  // Returns a shaped buffer containing the result of the computation.\n  StatusOr&lt;ScopedShapedBuffer&gt; ExecuteOnStream(\n      const ServiceExecutableRunOptions* run_options,\n      absl::Span&lt;const ShapedBuffer* const&gt; arguments,\n      HloExecutionProfile* hlo_execution_profile);\n</code></pre> <p>Note</p> <ul> <li>ServiceExecutableRunOptions contains Stream and options for running Executable.</li> </ul>"},{"location":"structure/HloComputation/","title":"HloComputation","text":"<p>You can think of an HloComputation like a function.  It has some inputs (parameters) and returns exactly one value (the value of its root node).  If you want to return multiple values, you can return a tuple. The instructions inside of a computation do not have an explicit total order. Instead, they have a partial order determined by their data and control dependencies.</p>"},{"location":"structure/HloComputation/#attributes-overview","title":"Attributes Overview","text":"<ul> <li>Store and access HloInstructions in HloComputation.</li> </ul> <pre><code>  HloInstruction* root_instruction_;\n\n  // If this computation is a fusion computation, this field points to the\n  // corresponding fusion instruction (if it is live). Otherwise, this is null.\n  HloInstruction* fusion_instruction_;\n\n  // Determines whether this computation is a fusion computation. A fusion\n  // computation ordinarily also has a non-null fusion_instruction_. However, if\n  // a fusion instruction is removed during compilation, the fusion computation\n  // becomes unreachable, and its fusion_instruction_ is set to null. We still\n  // need to regard such computations as fusion computations for HLO scheduling\n  // purposes.\n  bool is_fusion_computation_;\n\n  // Module containing this computation.\n  HloModule* parent_ = nullptr;\n\n  // Store instructions in std::list as they can be added and removed\n  // arbitrarily and we want a stable iteration order. Keep a map from\n  // instruction pointer to location in the list for fast lookup.\n  InstructionList instructions_;\n  absl::flat_hash_map&lt;const HloInstruction*, InstructionList::iterator&gt;\n      instruction_iterators_;\n\n  HloInstruction::InstructionVector param_instructions_;\n</code></pre> <p>Note</p> <ul> <li>Root instruction is the last instruction of the computation.</li> <li>Parameter instructions are the inputs to the computation.</li> </ul>"},{"location":"structure/HloComputation/#functions-overview","title":"Functions Overview","text":"<ul> <li>Build HloComputation from HloInstructions.</li> <li>Perform operations on HloInstruction.</li> </ul> <pre><code>  // Build and return an HloComputation. The parameter root_instruction\n  // specifies the already-added instruction to use as the root. If\n  // root_instruction is nullptr then use the last added instruction as the\n  // root.\n  std::unique_ptr&lt;HloComputation&gt; Build(\n      HloInstruction* root_instruction = nullptr);\n\n  // Add an instruction to the computation. The computation takes ownership of\n  // the instruction.\n  HloInstruction* AddInstruction(std::unique_ptr&lt;HloInstruction&gt; instruction,\n                                 absl::string_view new_name = \"\");\n\n  // Remove an instruction from the computation. The instruction must have no\n  // users. Instruction is deallocated with this call.\n  Status RemoveInstruction(HloInstruction* instruction);\n\n  // Replaces old instruction with newly created instruction. Removes old\n  // instruction from computation. Updates uses and root instruction.\n  Status ReplaceWithNewInstruction(\n      HloInstruction* old_instruction,\n      std::unique_ptr&lt;HloInstruction&gt; new_instruction);\n\n  // Computes and returns the ProgramShape of this computation (shape of\n  // parameters and result with layout).\n  ProgramShape ComputeProgramShape(bool include_ids = true) const;\n</code></pre>"},{"location":"structure/HloEvaluator/","title":"HloEvaluator","text":"<p>Responsible for evaluating HLO and obtain literal as the evaluation results.</p> <p>Note</p> <ul> <li>HloEvaluator is used by interpreter to execute HLO operators.</li> <li>Backends (CPU/GPU) use Executable to execute HLO operators.</li> </ul> <p>Note</p> <ul> <li>Here, evaluating HLO means to actually perform the HLO operation.</li> </ul>"},{"location":"structure/HloEvaluator/#attributes-overview","title":"Attributes Overview","text":"<ul> <li>Store input and evaluated Literals</li> </ul> <pre><code>  // Tracks the HLO instruction and its evaluated literal result.\n  bsl::node_hash_map&lt;const HloInstruction*, Literal&gt; evaluated_;\n\n  // Caches pointers to input literals\n  std::vector&lt;const Literal*&gt; arg_literals_;\n</code></pre>"},{"location":"structure/HloEvaluator/#functions-overview","title":"Functions Overview","text":"<ul> <li>Evaluate HLO Operations and return result Literal</li> </ul> <pre><code>  // Evaluates an HLO module and an array of pointers to literals.  Returns the\n  // evaluated result as a literal if successful.\n  StatusOr&lt;Literal&gt; Evaluate(const HloModule&amp; module,\n                             absl::Span&lt;const Literal* const&gt; arg_literals) {\n    return Evaluate(*module.entry_computation(), arg_literals);\n  }\n\n  StatusOr&lt;Literal&gt; Evaluate(const HloComputation&amp; computation,\n                             absl::Span&lt;const Literal* const&gt; arg_literals);\n\n  StatusOr&lt;Literal&gt; Evaluate(\n      const HloInstruction* instruction,\n      bool recursively_evaluate_nonconstant_operands = false);\n\n  Status HandleConcatenate(const HloInstruction* concatenate) override;\n  Status HandleReshape(const HloInstruction* reshape) override;\n  Status HandleTranspose(const HloInstruction* transpose) override;\n</code></pre> <p>Note</p> <ul> <li>The result of evaluation is stored in <code>evaluated_</code></li> </ul>"},{"location":"structure/HloInstruction/","title":"HloInstruction","text":"<p>HLO instructions are the atomic unit of the high-level compiler's IR.</p>"},{"location":"structure/HloInstruction/#attributes-overview","title":"Attributes Overview","text":"<ul> <li>Stores HloInstruction type of operation, operators, users</li> <li>Which HloComputation it belongs to</li> <li>Shape of the result</li> </ul> <pre><code>  int unique_id_;  // Unique to this HloInstruction within a HloModule\n\n  // Opcode for this instruction.\n  HloOpcode opcode_;\n\n  // Instruction operands.\n  InstructionVector operands_;\n\n  // The set of control predecessors of this instruction.\n  std::vector&lt;HloInstruction*&gt; control_predecessors_;\n\n  // The users of this instruction. Users are HLOs where this instruction is an\n  // operand. The vector users_ and the map user_map_ contain identical members.\n  // The map enables fast membership testing and the vector enables fast, stable\n  // iteration. The value in the map contains the index of the instruction in\n  // the vector what enables fast removal.\n  std::vector&lt;HloInstruction*&gt; users_;\n  absl::flat_hash_map&lt;const HloInstruction*, int64_t&gt; user_map_;\n\n  // The set of control successors of this instruction.\n  std::vector&lt;HloInstruction*&gt; control_successors_;\n\n  // The computation in which this instruction is contained.\n  HloComputation* parent_ = nullptr;\n\n  // Result shape of this instruction.\n  Shape shape_;\n</code></pre>"},{"location":"structure/HloInstruction/#functions-overview","title":"Functions Overview","text":"<ul> <li>Create HloInstruction</li> <li>Modify attributes of HloInstruction</li> </ul> <pre><code>  // Creates a pad instruction, where the operand is padded on the edges and\n  // between the elements with the given padding value.\n  static std::unique_ptr&lt;HloInstruction&gt; CreatePad(\n      const Shape&amp; shape, HloInstruction* operand,\n      HloInstruction* padding_value, const PaddingConfig&amp; padding_config);\n\n  // Creates a reshape instruction, where the operand is flattened row-major\n  // order and then reshaped to the given result shape.\n  static std::unique_ptr&lt;HloInstruction&gt; CreateReshape(\n      const Shape&amp; shape, HloInstruction* operand,\n      int64_t inferred_dimension = -1);\n\n  // Creates a transpose instruction which permutes the operand dimensions.\n  static std::unique_ptr&lt;HloInstruction&gt; CreateTranspose(\n      const Shape&amp; shape, HloInstruction* operand,\n      absl::Span&lt;const int64_t&gt; dimensions);\n\n  // Replaces the use of this instruction in \"user\" with \"new_producer\". Note\n  // that there might be multiple uses of this instruction in \"user\"; all will\n  // be replaced.\n  Status ReplaceUseWith(HloInstruction* user, HloInstruction* new_producer);\n\n  // Replaces the specified operand with new_operand. The old and new operands\n  // must have compatible shapes ignoring floating-point precision.\n  Status ReplaceOperandWith(int64_t operand_num, HloInstruction* new_operand);\n\n  // Clones the HLO instruction. The clone will have the same opcode, shape, and\n  // operands.\n  std::unique_ptr&lt;HloInstruction&gt; Clone(\n      const std::string&amp; suffix = \"clone\",\n      HloCloneContext* context = nullptr) const;\n</code></pre>"},{"location":"structure/HloInstructionSequence/","title":"HloInstructionSequence","text":"<p>Class representing a sequence of HloInstructions for a HloComputation.</p>"},{"location":"structure/HloInstructionSequence/#attributes-overview","title":"Attributes Overview","text":"<ul> <li>Stores HloInstruction sequence for a HloComputation</li> </ul> <pre><code>  // The sequence as HloInstructions.\n  std::vector&lt;HloInstruction*&gt; instruction_sequence_;\n\n  // The sequence of HLO instructions, represented by their unique IDs. The\n  // sequence is stored as both HloInstructions and unique IDs because the\n  // sequence may be referenced after transformations to the HLO graph and HLO\n  // pointers can be invalidated or recycled in this process (see\n  // HloSchedule::Update).\n  std::vector&lt;int&gt; id_sequence_;\n</code></pre>"},{"location":"structure/HloInstructionSequence/#functions-overview","title":"Functions Overview","text":"<ul> <li>Modify sequence</li> </ul> <pre><code>  // Adds the instruction to the end of the sequence.\n  void push_back(HloInstruction* instruction);\n\n  // Removes the instruction from the sequence.\n  void remove_instruction(HloInstruction* instruction);\n\n  // Replaces the old instruction with the new instruction in the sequence.\n  void replace_instruction(HloInstruction* old_instruction,\n                           HloInstruction* new_instruction);\n</code></pre>"},{"location":"structure/HloModule/","title":"HloModule","text":"<p>HloModule is the top-level unit in the HLO IR.  It corresponds to a whole \"program\".  Running a module, from beginning to end, is the only way to run an XLA program. A module contains one \"entry computation\"; this HloComputation is like main() in a C program.  The result of running the module is the result of running this computation.</p>"},{"location":"structure/HloModule/#attributes-overview","title":"Attributes Overview","text":"<ul> <li>Store and access HloComputations in HloModule.</li> <li>HloSchedule to store sequential order of instructions.</li> </ul> <pre><code>  CopyOnWrite&lt;HloModuleConfig&gt; config_;\n  HloComputation* entry_computation_ = nullptr;\n  std::vector&lt;std::unique_ptr&lt;HloComputation&gt;&gt; computations_;\n\n  // The HloSchedule of the module. The schedule if it exists contains a\n  // sequential order of instructions for each non-fusion computation in the\n  // module.\n  std::optional&lt;HloSchedule&gt; schedule_;\n</code></pre>"},{"location":"structure/HloModule/#functions-overview","title":"Functions Overview","text":"<ul> <li>Perform operations on HloComputations present in HloModule.</li> </ul> <pre><code>  // Adds an entry computation to the module. A module can only have one entry\n  // computation. Returns a pointer to the newly added computation.\n  HloComputation* AddEntryComputation(\n      std::unique_ptr&lt;HloComputation&gt; computation);\n\n  // Performs a deep clone of the computation, by recursively cloning all\n  // the called computations as well. If the clone context is specified, it\n  // will be populated with the cloned object mappings.\n  HloComputation* DeepCloneComputation(HloComputation* computation,\n                                       HloCloneContext* context = nullptr);\n\n  // Replaces all uses of computations that are keys of 'replacements' with\n  // the corresponding values in 'replacements'. Replaces the entry computation,\n  // if applicable.\n  void ReplaceComputations(\n      const absl::flat_hash_map&lt;HloComputation*, HloComputation*&gt;&amp;\n          replacements);\n\n  // Returns the root instruction shape of entry computation.\n  // Precondition: entry_computation_ is not nullptr.\n  const Shape&amp; result_shape() const {\n    CHECK_NE(nullptr, entry_computation_);\n    return entry_computation()-&gt;root_instruction()-&gt;shape();\n  }\n</code></pre>"},{"location":"structure/HloSchedule/","title":"HloSchedule","text":"<p>A class representing a sequential schedule of instructions for an HloModule. A complete HLO schedule contains an instruction sequence for every non-fusion computation in the HloModule.</p>"},{"location":"structure/HloSchedule/#attributes-overview","title":"Attributes Overview","text":"<ul> <li>Stores sequence of HloInstructions for multiple HloComputation using HloInstructionSequence</li> </ul> <pre><code>  const HloModule* module_;\n\n  // A map from computation unique ID to instruction sequence. Unique IDs are\n  // used rather than HloComputation pointers because HLO pointers are not\n  // unique across HLO transformations because pointers may be recycled.\n  absl::flat_hash_map&lt;int64_t, HloInstructionSequence&gt; sequences_;\n</code></pre>"},{"location":"structure/HloSchedule/#functions-overview","title":"Functions Overview","text":"<ul> <li>Modify the sequence of HloInstructions</li> </ul> <pre><code>  // Sets the sequence for the given computation to the given sequence.\n  void set_sequence(const HloComputation* computation,\n                    absl::Span&lt;HloInstruction* const&gt; sequence);\n\n  // Removes the instruction from the computation's sequence.\n  void remove_instruction(const HloComputation* computation,\n                          HloInstruction* instruction) {\n    sequences_[computation-&gt;unique_id()].remove_instruction(instruction);\n  }\n\n  // Replaces the old instruction with the new instruction in the computation's\n  // sequence.\n  void replace_instruction(const HloComputation* computation,\n                           HloInstruction* old_instruction,\n                           HloInstruction* new_instruction) {\n    sequences_[computation-&gt;unique_id()].replace_instruction(old_instruction,\n                                                             new_instruction);\n  }\n</code></pre>"},{"location":"structure/Kernel/","title":"Kernel","text":"<p>A data-parallel kernel (code entity) for launching via the StreamExecutor.</p>"},{"location":"structure/Kernel/#attributes-overview","title":"Attributes Overview","text":"<pre><code>  // The StreamExecutor that loads this kernel object.\n  StreamExecutor *parent_;\n\n  // Implementation delegated to for platform-specific functionality.\n  std::unique_ptr&lt;internal::KernelInterface&gt; implementation_;\n</code></pre>"},{"location":"structure/Layout/","title":"Layout","text":"<p>Defines layout</p>"},{"location":"structure/Layout/#attributes-overview","title":"Attributes Overview","text":"<ul> <li>Stores layout</li> </ul> <pre><code>  // A map from physical dimension numbers to logical dimension numbers.\n  // The first element is the most minor physical dimension (fastest varying\n  // index) and the last the most major (slowest varying index). The contents of\n  // the vector are the indices of the *logical* dimensions in the shape.\n  //\n  // For example, in shape f32[8,100,100,3]{3,0,2,1}, the logical dimensions\n  // are [8,100,100,3] and minor_to_major_ is {3,0,2,1}.\n  // So, the most minor physical dimension is [8,100,100,3][3], which is size 3.\n  // The second most minor is [8,100,100,3][0], which is size 8.\n  // The third most minor is [8,100,100,3][2], which is size 100.\n  // And the major dim is [8,100,100,3][1], which is size 100.\n  DimensionVector minor_to_major_;\n</code></pre>"},{"location":"structure/Literal/","title":"Literal","text":"<p>Actual data value stored in host.</p>"},{"location":"structure/ProgramShape/","title":"ProgramShape","text":"<p>Shape of the parameters and output of an XLA computation.</p>"},{"location":"structure/ProgramShape/#attributes-overview","title":"Attributes Overview","text":"<pre><code>  // The shapes of the parameters of the computation represented by this object.\n  std::vector&lt;Shape&gt; parameters_;\n\n  // The names of the parameters of the computation represented by this object.\n  std::vector&lt;std::string&gt; parameter_names_;\n\n  // The shape of the result of the computation represented by this object.\n  Shape result_;\n</code></pre>"},{"location":"structure/Service/","title":"Service","text":"<p>The XLA service object, which is the same across all platforms. It maintains the service state of computations and allocations, and delegates target-specific requests to the target-specific infrastructure (target-specific compiler, StreamExecutor).</p>"},{"location":"structure/Service/#attributes-overview","title":"Attributes Overview","text":"<ul> <li>Cache build Executables</li> <li>Tracks the stage of execution</li> </ul> <pre><code>  ServiceOptions options_;\n\n  // Cache containing previously built Executables.\n  CompilationCache compilation_cache_;\n\n  // Tracks asynchronously launched executions via the API.\n  ExecutionTracker execution_tracker_;\n</code></pre>"},{"location":"structure/Service/#functions-overview","title":"Functions Overview","text":"<ul> <li>Compiles and Executes XlaComputation</li> <li>Build StreamExecutor and Executable</li> <li>Transfer data to and from Clinet</li> </ul> <pre><code>  // Compiles a computation into an executable. The request contains the whole\n  // computation graph. Returns the handle to the executable.\n  Status Compile(const CompileRequest* arg, CompileResponse* result) override;\n\n  // Executes an executable with the provided global data passes as immutable\n  // arguments. The request contains the handle to the executable. Returns\n  // global data output and execution timing.\n  Status Execute(const ExecuteRequest* arg, ExecuteResponse* result) override;\n\n  // Requests that global data be transferred to the client in literal form.\n  Status TransferToClient(const TransferToClientRequest* arg,\n                          TransferToClientResponse* result) override;\n\n  // Transfers data from a literal provided by the client, into device memory.\n  Status TransferToServer(const TransferToServerRequest* arg,\n                          TransferToServerResponse* result) override;\n\n  // Prepare the executors for executing parallel.\n  StatusOr&lt;std::vector&lt;se::StreamExecutor*&gt;&gt; GetExecutors(\n      const ExecutionOptions&amp; execution_options, int64_t requests_size,\n      int64_t request_index) const;\n\n  // Returns the stream executors assigned to the replicas represented by the\n  // given device handle. Each device_handle is a virtual replicated device that\n  // represents a set of physical devices for the replicas.\n  StatusOr&lt;std::vector&lt;se::StreamExecutor*&gt;&gt; Replicas(\n      const Backend&amp; backend, const DeviceHandle&amp; device_handle) const;\n\n  // Builds an Executable for the given parameters.\n  StatusOr&lt;std::unique_ptr&lt;Executable&gt;&gt; BuildExecutable(\n      const HloModuleProto&amp; module_proto,\n      std::unique_ptr&lt;HloModuleConfig&gt; module_config, Backend* backend,\n      se::StreamExecutor* executor, const Compiler::CompileOptions&amp; options,\n      bool run_backend_only = false);\n</code></pre>"},{"location":"structure/Shape/","title":"Shape","text":"<p>A shape describes the number of dimensions in a array, the bounds of each dimension, and the primitive component type.</p>"},{"location":"structure/Shape/#attributes-overview","title":"Attributes Overview","text":"<ul> <li>Stores details about element type, dimensions and Layout</li> </ul> <pre><code>  // The element type of this shape (tuple, array, etc).\n  PrimitiveType element_type_ = PRIMITIVE_TYPE_INVALID;\n\n  // The array bounds of the dimensions. This is nonempty only for array\n  // shapes. For a dynamically-sized dimension, the respective value in this\n  // vector is an inclusive upper limit of the array bound.\n  DimensionVector dimensions_;\n\n  // This vector is the same size as 'dimensions_' and indicates whether the\n  // respective dimension is dynamically sized.\n  absl::InlinedVector&lt;bool, InlineRank()&gt; dynamic_dimensions_;\n\n  // The tuple element subshapes. This is nonempty only for tuple shapes.\n  std::vector&lt;Shape&gt; tuple_shapes_;\n\n  // The layout of the shape. Only relevant for arrays.\n  std::optional&lt;Layout&gt; layout_;\n</code></pre>"},{"location":"structure/Shape/#functions-overview","title":"Functions Overview","text":"<pre><code>  // Prints a human-readable string that represents the given shape, with or\n  // without layout. e.g. \"F32[42,12] {0, 1}\" or \"F32[64]\".\n  void Print(Printer* printer, bool print_layout = false) const;\n</code></pre>"},{"location":"structure/Stream/","title":"Stream","text":"<p>Warning</p> <ul> <li>Still have doubts. Requires further study.</li> </ul> <p>Question</p> <ul> <li>Does each Stream end up being a set of Kernels down the line?</li> <li>Is Stream just a predefined set of dependent Kernels?</li> </ul> <p>Represents a stream of dependent computations on a GPU device.</p> <p>The operations within a stream execute linearly and asynchronously until BlockHostUntilDone() is invoked, which synchronously joins host code with the execution of the stream.</p>"},{"location":"structure/Stream/#attributes-overview","title":"Attributes Overview","text":"<ul> <li>Stores StreamExecutor which executes Stream on device</li> </ul> <pre><code>  // The StreamExecutor that supports the operation of this stream.\n  StreamExecutor *parent_;\n</code></pre>"},{"location":"structure/Stream/#functions-overview","title":"Functions Overview","text":"<ul> <li>Define Streams</li> <li>Entrains Kernels into Stream</li> </ul> <pre><code>  // Initialize the stream. This must be performed before entraining any other\n  // operations.\n  Stream &amp;Init() TF_LOCKS_EXCLUDED(mu_);\n\n  // Entrains onto the stream of operations: a kernel launch with the given\n  // (variadic) parameters for the invocation.\n  template &lt;typename... Params, typename... Args&gt;\n  tsl::Status ThenLaunch(ThreadDim thread_dims, BlockDim block_dims,\n                         const TypedKernel&lt;Params...&gt; &amp;kernel, Args... args);\n\n  Stream &amp;ThenConvolve(const dnn::BatchDescriptor &amp;input_descriptor,\n                       const DeviceMemory&lt;float&gt; &amp;input_data,\n                       const dnn::FilterDescriptor &amp;filter_descriptor,\n                       const DeviceMemory&lt;float&gt; &amp;filter_data,\n                       const dnn::ConvolutionDescriptor &amp;convolution_descriptor,\n                       const dnn::BatchDescriptor &amp;output_descriptor,\n                       DeviceMemory&lt;float&gt; *output);\n\n  Stream &amp;ThenMatMul(const DeviceMemory&lt;float&gt; &amp;input_data,\n                     const DeviceMemory&lt;float&gt; &amp;weights,\n                     const dnn::BatchDescriptor &amp;input_dimensions,\n                     const dnn::BatchDescriptor &amp;output_dimensions,\n                     DeviceMemory&lt;float&gt; *output_data);\n</code></pre>"},{"location":"structure/StreamExecutor/","title":"StreamExecutor","text":"<p>A StreamExecutor manages a single device, in terms of executing work (kernel launches) and memory management (allocation/deallocation, memory copies to and from the device). It is conceptually the \"handle\" for a device.</p>"},{"location":"structure/StreamExecutor/#attributes-overview","title":"Attributes Overview","text":"<pre><code>  // Reference to the platform that created this executor.\n  const Platform* platform_;\n\n  // Pointer to the platform-specific-interface implementation. This is\n  // delegated to by the interface routines in pointer-to-implementation\n  // fashion.\n  std::unique_ptr&lt;internal::StreamExecutorInterface&gt; implementation_;\n</code></pre>"},{"location":"structure/StreamExecutor/#functions-overview","title":"Functions Overview","text":"<ul> <li>Load Kernel</li> <li>Perform device memory management</li> </ul> <pre><code>  // Retrieves (loads) a kernel for the platform this StreamExecutor is acting\n  // upon, if one exists.\n  tsl::Status GetKernel(const MultiKernelLoaderSpec&amp; spec, Kernel* kernel);\n\n  // Loads a module for the platform this StreamExecutor is acting upon.\n  tsl::Status LoadModule(const MultiModuleLoaderSpec&amp; spec,\n                         ModuleHandle* module_handle);\n\n  // Synchronously allocates an array on the device of type T with element_count\n  // elements.\n  template &lt;typename T&gt;\n  DeviceMemory&lt;T&gt; AllocateArray(uint64_t element_count,\n                                int64_t memory_space = 0);\n\n  // Warning: use Stream::ThenLaunch instead, this method is not for general\n  // consumption. However, this is the only way to launch a kernel for which\n  // the type signature is only known at runtime; say, if an application\n  // supports loading/launching kernels with arbitrary type signatures.\n  // In this case, the application is expected to know how to do parameter\n  // packing that obeys the contract of the underlying platform implementation.\n  //\n  // Launches a data parallel kernel with the given thread/block\n  // dimensionality and already-packed args/sizes to pass to the underlying\n  // platform driver.\n  //\n  // This is called by Stream::Launch() to delegate to the platform's launch\n  // implementation in StreamExecutorInterface::Launch().\n  tsl::Status Launch(Stream* stream, const ThreadDim&amp; thread_dims,\n                     const BlockDim&amp; block_dims, const Kernel&amp; kernel,\n                     const KernelArgs&amp; args);\n\n  // Submits command buffer for execution to the underlying platform driver.\n  tsl::Status Submit(Stream* stream, const CommandBuffer&amp; command_buffer);\n</code></pre>"},{"location":"structure/XlaBuilder/","title":"XlaBuilder","text":"<p>A convenient interface for building up computations.</p> <p>Note</p> <p>This is the class which build XLA graph (XlaComputation)</p>"},{"location":"structure/XlaBuilder/#attributes-overview","title":"Attributes Overview","text":"<ul> <li>Stores instructions used to build XlaComputation</li> </ul> <pre><code>  // The instructions of this computation.\n  std::deque&lt;HloInstructionProto&gt; instructions_;\n\n  // A map from XlaOp::Handle to the index in the instructions_ vector where the\n  // instruction is held.\n  absl::flat_hash_map&lt;int64_t, int64_t&gt; handle_to_index_;\n\n  // The unique parameter numbers.\n  absl::flat_hash_set&lt;int64_t&gt; parameter_numbers_;\n</code></pre>"},{"location":"structure/XlaBuilder/#functions-overview","title":"Functions Overview","text":"<ul> <li>Constructs XlaOps</li> <li>Build XlaComputation</li> <li>Get computation details like ProgramShape</li> </ul> <pre><code>  XlaOp Reshape(XlaOp operand, absl::Span&lt;const int64_t&gt; new_sizes,\n                int64_t inferred_dimension = -1);\n\n  XlaOp Slice(XlaOp operand, absl::Span&lt;const int64_t&gt; start_indices,\n              absl::Span&lt;const int64_t&gt; limit_indices,\n              absl::Span&lt;const int64_t&gt; strides);\n\n  XlaOp Conv(\n      XlaOp lhs, XlaOp rhs, absl::Span&lt;const int64_t&gt; window_strides,\n      Padding padding, int64_t feature_group_count = 1,\n      int64_t batch_group_count = 1,\n      const PrecisionConfig* precision_config = nullptr,\n      std::optional&lt;PrimitiveType&gt; preferred_element_type = std::nullopt);\n\n  // Builds the computation with the requested operations, or returns a non-ok\n  // status. Note that all ops that have been enqueued will be moved to the\n  // computation being returned. The root of the computation will be the last\n  // added operation.\n  StatusOr&lt;XlaComputation&gt; Build(bool remove_dynamic_dimensions = false);\n\n  // Returns the shape of the given op.\n  StatusOr&lt;Shape&gt; GetShape(XlaOp op) const;\n\n  // Returns the (inferred) result for the current computation's shape. This\n  // assumes the root instruction is the last added instruction.\n  StatusOr&lt;ProgramShape&gt; GetProgramShape() const;\n</code></pre>"},{"location":"structure/XlaComputation/","title":"XlaComputation","text":"<p>The computation graph that the user builds up with the XlaBuilder.</p>"},{"location":"structure/XlaComputation/#attributes-overview","title":"Attributes Overview","text":"<pre><code>  int64_t unique_id_;\n  HloModuleProto proto_;\n</code></pre> <p>Note</p> <ul> <li>HloModuleProto stores all information that HloModule contains.</li> <li>So, information like HloComputation, entry computation is stored</li> </ul>"},{"location":"structure/XlaOp/","title":"XlaOp","text":"<p>This represents an instruction that has been enqueued using the XlaBuilder. This is used to pass to subsequent computations that depends upon the instruction as an operand.</p>"},{"location":"structure/XlaOp/#attributes-overview","title":"Attributes Overview","text":"<pre><code>  // &lt; 0 means \"invalid handle\".\n  int64_t handle_;\n\n  // Not owned. Non-null for any handle returned by XlaBuilder, even if the\n  // handle is invalid.\n  XlaBuilder* builder_;\n</code></pre> <p>Note</p> <p>handle_ is used to point to its corresponding HloInstructionProto via handle_to_index_ in XlaBuilder.</p>"}]}